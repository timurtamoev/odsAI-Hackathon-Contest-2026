{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40201b99",
      "metadata": {},
      "source": [
        "# Book Recommendation Hackathon — Professional Solution\n",
        "\n",
        "**Task:** Rank 20 editions for each user from 200 candidates, optimizing Score = 0.7×NDCG@20 + 0.3×Diversity@20\n",
        "\n",
        "**Strategy:** Two-stage pipeline — (1) Relevance model (LightGBM ranker) → (2) Diversity-aware re-ranking (MMR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7afc16f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 1. Imports & Config\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Paths - adjust if your data is elsewhere\n",
        "BASE = Path(\".\")\n",
        "DATA_DIR = BASE / \"data\"\n",
        "SUBMIT_DIR = BASE / \"submit\"\n",
        "SUBMISSION_PATH = BASE / \"submission.csv\"\n",
        "\n",
        "# Hackathon params\n",
        "ALPHA = 0.7  # NDCG weight in final score\n",
        "BETA = 0.5   # Coverage vs ILD in Diversity\n",
        "N_TOP = 20\n",
        "N_CANDIDATES = 200\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d37aaf2b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "users.csv: (5067, 3)\n",
            "interactions.csv: (231210, 5)\n",
            "editions.csv: (134231, 9)\n",
            "authors.csv: (36360, 2)\n",
            "genres.csv: (586, 2)\n",
            "book_genres.csv: (168251, 2)\n",
            "targets: (5067, 1), candidates: (1013400, 2)\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# 2. Data Loading\n",
        "\n",
        "# %%\n",
        "def load_data(data_dir: Path):\n",
        "    \"\"\"Load all CSV files. Returns dict of DataFrames.\"\"\"\n",
        "    data = {}\n",
        "    for name in [\"users\", \"interactions\", \"editions\", \"authors\", \"genres\", \"book_genres\"]:\n",
        "        p = data_dir / f\"{name}.csv\"\n",
        "        if p.exists():\n",
        "            data[name] = pd.read_csv(p)\n",
        "        else:\n",
        "            data[name] = None\n",
        "    targets = pd.read_csv(SUBMIT_DIR / \"targets.csv\")\n",
        "    candidates = pd.read_csv(SUBMIT_DIR / \"candidates.csv\")\n",
        "    return data, targets, candidates\n",
        "\n",
        "data, targets, candidates = load_data(DATA_DIR)\n",
        "for k, v in data.items():\n",
        "    if v is not None:\n",
        "        print(f\"{k}.csv: {v.shape}\")\n",
        "print(f\"targets: {targets.shape}, candidates: {candidates.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5b1ee24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 3. Evaluation Metrics (NDCG@20 & Diversity@20)\n",
        "\n",
        "# %%\n",
        "def compute_ndcg_at_k(relevances: np.ndarray, k: int = 20) -> float:\n",
        "    \"\"\"NDCG@k for one user. relevances: array of rel per position (0/1/3).\"\"\"\n",
        "    relevances = np.asarray(relevances[:k], dtype=float)\n",
        "    if len(relevances) < k:\n",
        "        relevances = np.pad(relevances, (0, k - len(relevances)))\n",
        "    dcg = np.sum(relevances / np.log2(np.arange(2, k + 2)))\n",
        "    ideal = np.sort(relevances)[::-1]\n",
        "    idcg = np.sum(ideal / np.log2(np.arange(2, k + 2)))\n",
        "    if idcg <= 0:\n",
        "        return 0.0\n",
        "    return dcg / idcg\n",
        "\n",
        "\n",
        "def compute_coverage_at_k(\n",
        "    ranked_editions: List[int],\n",
        "    rel_binary: np.ndarray,\n",
        "    edition_to_genres: Dict[int, set],\n",
        "    k: int = 20\n",
        ") -> float:\n",
        "    \"\"\"Genre coverage (relevance-weighted). Only counts positions with rel>0.\"\"\"\n",
        "    w_sum = sum(1.0 / np.log2(i + 2) for i in range(k))\n",
        "    s = set()\n",
        "    score = 0.0\n",
        "    for pos in range(k):\n",
        "        w_k = 1.0 / np.log2(pos + 2)\n",
        "        ed = ranked_editions[pos] if pos < len(ranked_editions) else None\n",
        "        if ed is None or rel_binary[pos] == 0:\n",
        "            continue\n",
        "        g = edition_to_genres.get(ed, set())\n",
        "        if not g:\n",
        "            continue\n",
        "        new_genres = len(g - s)\n",
        "        s |= g\n",
        "        score += w_k * (new_genres / len(g))\n",
        "    return score / w_sum if w_sum > 0 else 0.0\n",
        "\n",
        "\n",
        "def jaccard_distance(g1: set, g2: set) -> float:\n",
        "    if not g1 or not g2:\n",
        "        return 0.0\n",
        "    inter = len(g1 & g2)\n",
        "    union = len(g1 | g2)\n",
        "    return 1.0 - (inter / union) if union > 0 else 0.0\n",
        "\n",
        "\n",
        "def compute_ild_at_k(\n",
        "    ranked_editions: List[int],\n",
        "    rel_binary: np.ndarray,\n",
        "    edition_to_genres: Dict[int, set],\n",
        "    k: int = 20\n",
        ") -> float:\n",
        "    \"\"\"Intra-list diversity: avg Jaccard distance among relevant items.\"\"\"\n",
        "    L = [ranked_editions[i] for i in range(min(k, len(ranked_editions))) if rel_binary[i] == 1]\n",
        "    if len(L) < 2:\n",
        "        return 0.0\n",
        "    total = 0.0\n",
        "    n_pairs = 0\n",
        "    for i in range(len(L)):\n",
        "        for j in range(i + 1, len(L)):\n",
        "            g1 = edition_to_genres.get(L[i], set())\n",
        "            g2 = edition_to_genres.get(L[j], set())\n",
        "            total += jaccard_distance(g1, g2)\n",
        "            n_pairs += 1\n",
        "    return total / n_pairs if n_pairs else 0.0\n",
        "\n",
        "\n",
        "def diversity_at_k(\n",
        "    ranked_editions: List[int],\n",
        "    rel_binary: np.ndarray,\n",
        "    edition_to_genres: Dict[int, set],\n",
        "    k: int = 20,\n",
        "    beta: float = 0.5\n",
        ") -> float:\n",
        "    cov = compute_coverage_at_k(ranked_editions, rel_binary, edition_to_genres, k)\n",
        "    ild = compute_ild_at_k(ranked_editions, rel_binary, edition_to_genres, k)\n",
        "    return beta * cov + (1 - beta) * ild\n",
        "\n",
        "\n",
        "def evaluate_submission(\n",
        "    submission: pd.DataFrame,\n",
        "    ground_truth: Dict[Tuple[int, int], int],  # (user_id, edition_id) -> rel\n",
        "    edition_to_genres: Dict[int, set],\n",
        "    alpha: float = 0.7,\n",
        "    beta: float = 0.5\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    submission: user_id, edition_id, rank\n",
        "    ground_truth: map (user_id, edition_id) -> relevance (0/1/3)\n",
        "    \"\"\"\n",
        "    users = submission[\"user_id\"].unique()\n",
        "    ndcgs, divs = [], []\n",
        "    for uid in users:\n",
        "        rows = submission[submission[\"user_id\"] == uid].sort_values(\"rank\")\n",
        "        editions = rows[\"edition_id\"].tolist()\n",
        "        rel = np.array([ground_truth.get((uid, e), 0) for e in editions])\n",
        "        rel_bin = (rel > 0).astype(int)\n",
        "        ndcgs.append(compute_ndcg_at_k(rel, N_TOP))\n",
        "        divs.append(diversity_at_k(editions, rel_bin, edition_to_genres, N_TOP, beta))\n",
        "    n_avg = np.mean(ndcgs)\n",
        "    d_avg = np.mean(divs)\n",
        "    score = alpha * n_avg + (1 - alpha) * d_avg\n",
        "    return score, n_avg, d_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "478b86d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "edition_to_genres: 134231 editions\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# 4. Build Edition→Genres & Metadata Mappings\n",
        "\n",
        "# %%\n",
        "def build_edition_genres(editions: pd.DataFrame, book_genres: pd.DataFrame) -> Dict[int, set]:\n",
        "    \"\"\"Map edition_id -> set of genre_ids (via book_id).\"\"\"\n",
        "    if editions is None or book_genres is None:\n",
        "        return {}\n",
        "    e2b = dict(zip(editions[\"edition_id\"], editions[\"book_id\"]))\n",
        "    bg = book_genres.groupby(\"book_id\")[\"genre_id\"].apply(set).to_dict()\n",
        "    return {eid: bg.get(bid, set()) for eid, bid in e2b.items()}\n",
        "\n",
        "\n",
        "def build_edition_metadata(editions: pd.DataFrame, authors: pd.DataFrame):\n",
        "    \"\"\"edition_id -> book_id, author_id, publication_year, etc.\"\"\"\n",
        "    if editions is None:\n",
        "        return {}\n",
        "    meta = editions.set_index(\"edition_id\").to_dict(\"index\")\n",
        "    return meta\n",
        "\n",
        "edition_to_genres = build_edition_genres(data[\"editions\"], data[\"book_genres\"])\n",
        "edition_meta = build_edition_metadata(data[\"editions\"], data[\"authors\"])\n",
        "print(f\"edition_to_genres: {len(edition_to_genres)} editions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bf95ebb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 5. Feature Engineering\n",
        "\n",
        "# %%\n",
        "def prepare_train_data(\n",
        "    interactions: pd.DataFrame,\n",
        "    editions: pd.DataFrame,\n",
        "    book_genres: pd.DataFrame,\n",
        "    edition_to_genres: Dict,\n",
        "    val_ratio: float = 0.2,\n",
        "    min_interactions_for_user: int = 5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Temporal split: last `val_ratio` of time for validation.\n",
        "    Create (user, edition) pairs with relevance label for training.\n",
        "    \"\"\"\n",
        "    if interactions is None or interactions.empty:\n",
        "        return None, None\n",
        "    inter = interactions.copy()\n",
        "    inter[\"event_ts\"] = pd.to_datetime(inter[\"event_ts\"])\n",
        "    inter = inter.sort_values(\"event_ts\")\n",
        "    \n",
        "    # relevance: read=3, wishlist=1\n",
        "    inter[\"rel\"] = inter[\"event_type\"].map({2: 3, 1: 1}).fillna(0).astype(int)\n",
        "    inter = inter.sort_values([\"user_id\", \"event_ts\"]).drop_duplicates(\n",
        "        subset=[\"user_id\", \"edition_id\"], keep=\"last\"\n",
        "    )\n",
        "    \n",
        "    # per user: max relevance (read overwrites wishlist)\n",
        "    inter = inter.groupby([\"user_id\", \"edition_id\"], as_index=False)[\"rel\"].max()\n",
        "    \n",
        "    # temporal split\n",
        "    t_max = inter[\"event_ts\"].max() if \"event_ts\" in inter.columns else None\n",
        "    if t_max is not None:\n",
        "        cutoff = inter[\"event_ts\"].quantile(1 - val_ratio)\n",
        "        train_inter = inter[inter[\"event_ts\"] < cutoff]\n",
        "        val_inter = inter[inter[\"event_ts\"] >= cutoff]\n",
        "    else:\n",
        "        # fallback: random split per user\n",
        "        users = inter[\"user_id\"].unique()\n",
        "        np.random.shuffle(users)\n",
        "        n_val = int(len(users) * val_ratio)\n",
        "        val_users = set(users[:n_val])\n",
        "        train_inter = inter[~inter[\"user_id\"].isin(val_users)]\n",
        "        val_inter = inter[inter[\"user_id\"].isin(val_users)]\n",
        "    \n",
        "    return train_inter, val_inter\n",
        "\n",
        "\n",
        "def build_features(\n",
        "    user_edition_pairs: pd.DataFrame,\n",
        "    interactions: pd.DataFrame,\n",
        "    edition_to_genres: Dict,\n",
        "    edition_meta: dict,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    user_edition_pairs: user_id, edition_id\n",
        "    Returns DataFrame with features + label if present.\n",
        "    \"\"\"\n",
        "    df = user_edition_pairs.copy()\n",
        "    if interactions is None or interactions.empty:\n",
        "        return df\n",
        "    \n",
        "    inter = interactions\n",
        "    # User stats\n",
        "    u_counts = inter.groupby(\"user_id\").agg(\n",
        "        user_total=(\"edition_id\", \"count\"),\n",
        "        user_reads=(\"event_type\", lambda x: (x == 2).sum()),\n",
        "        user_wishlists=(\"event_type\", lambda x: (x == 1).sum()),\n",
        "    ).reset_index()\n",
        "    df = df.merge(u_counts, on=\"user_id\", how=\"left\")\n",
        "    \n",
        "    # Item stats (edition popularity)\n",
        "    i_counts = inter.groupby(\"edition_id\").agg(\n",
        "        item_total=(\"user_id\", \"count\"),\n",
        "        item_reads=(\"event_type\", lambda x: (x == 2).sum()),\n",
        "        item_wishlists=(\"event_type\", lambda x: (x == 1).sum()),\n",
        "    ).reset_index()\n",
        "    df = df.merge(i_counts, on=\"edition_id\", how=\"left\")\n",
        "    \n",
        "    # User-item: did user interact before?\n",
        "    ui = inter[[\"user_id\", \"edition_id\"]].drop_duplicates()\n",
        "    ui[\"user_item_interacted\"] = 1\n",
        "    df = df.merge(ui, on=[\"user_id\", \"edition_id\"], how=\"left\")\n",
        "    df[\"user_item_interacted\"] = df[\"user_item_interacted\"].fillna(0)\n",
        "    \n",
        "    # Fill NaN\n",
        "    for c in [\"user_total\", \"user_reads\", \"user_wishlists\", \"item_total\", \"item_reads\", \"item_wishlists\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(0)\n",
        "    \n",
        "    # Genre overlap: user's preferred genres vs item genres\n",
        "    if edition_to_genres:\n",
        "        u_genres = inter.merge(\n",
        "            pd.DataFrame([\n",
        "                (eid, gid) for eid, gs in edition_to_genres.items() for gid in gs\n",
        "            ], columns=[\"edition_id\", \"genre_id\"]),\n",
        "            on=\"edition_id\"\n",
        "        ).groupby(\"user_id\")[\"genre_id\"].apply(set).to_dict()\n",
        "        def genre_overlap(row):\n",
        "            ug = u_genres.get(row[\"user_id\"], set())\n",
        "            ig = edition_to_genres.get(row[\"edition_id\"], set())\n",
        "            if not ug or not ig:\n",
        "                return 0.0\n",
        "            return len(ug & ig) / len(ug | ig) if (ug | ig) else 0.0\n",
        "        df[\"genre_overlap\"] = df.apply(genre_overlap, axis=1)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e3d16ced",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 6. Relevance Model (LightGBM)\n",
        "\n",
        "# %%\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except ImportError:\n",
        "    HAS_LGB = False\n",
        "    print(\"Install lightgbm: pip install lightgbm\")\n",
        "\n",
        "FEAT_COLS = [\n",
        "    \"user_total\", \"user_reads\", \"user_wishlists\",\n",
        "    \"item_total\", \"item_reads\", \"item_wishlists\",\n",
        "    \"user_item_interacted\", \"genre_overlap\"\n",
        "]\n",
        "\n",
        "\n",
        "def create_training_pairs(interactions: pd.DataFrame, edition_to_genres: dict, n_neg_per_user: int = 50):\n",
        "    \"\"\"\n",
        "    Create labeled (user, edition) pairs. Positives from interactions, negatives sampled.\n",
        "    \"\"\"\n",
        "    if interactions is None or interactions.empty:\n",
        "        return None\n",
        "    inter = interactions.copy()\n",
        "    inter[\"event_ts\"] = pd.to_datetime(inter[\"event_ts\"])\n",
        "    inter[\"rel\"] = inter[\"event_type\"].map({2: 3, 1: 1}).fillna(0)\n",
        "    inter = inter.groupby([\"user_id\", \"edition_id\"], as_index=False).agg(\n",
        "        rel=(\"rel\", \"max\"), event_ts=(\"event_ts\", \"max\")\n",
        "    )\n",
        "    \n",
        "    all_editions = list(edition_to_genres.keys()) if edition_to_genres else inter[\"edition_id\"].unique().tolist()\n",
        "    if not all_editions:\n",
        "        all_editions = inter[\"edition_id\"].unique().tolist()\n",
        "    pop = inter[\"edition_id\"].value_counts()\n",
        "    neg_pool = pop.index.tolist()[:5000] or all_editions[:min(5000, len(all_editions))]\n",
        "    \n",
        "    positives = inter[inter[\"rel\"] > 0][[\"user_id\", \"edition_id\", \"rel\"]]\n",
        "    neg_pairs = []\n",
        "    for uid, g in inter.groupby(\"user_id\"):\n",
        "        seen = set(g[\"edition_id\"])\n",
        "        cand = [e for e in neg_pool if e not in seen][:n_neg_per_user]\n",
        "        for eid in cand:\n",
        "            neg_pairs.append({\"user_id\": uid, \"edition_id\": eid, \"rel\": 0})\n",
        "    \n",
        "    neg_df = pd.DataFrame(neg_pairs)\n",
        "    train_df = pd.concat([positives, neg_df], ignore_index=True)\n",
        "    return train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b83501e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 7. Train Ranker & Predict\n",
        "\n",
        "# %%\n",
        "def train_ranker(\n",
        "    train_df: pd.DataFrame,\n",
        "    interactions: pd.DataFrame,\n",
        "    edition_to_genres: Dict,\n",
        "    edition_meta: dict,\n",
        "    feats: List[str] = None,\n",
        "):\n",
        "    feats = feats or FEAT_COLS\n",
        "    X = build_features(train_df, interactions, edition_to_genres, edition_meta)\n",
        "    available = [f for f in feats if f in X.columns]\n",
        "    X = X[[\"user_id\", \"edition_id\"] + available].sort_values(\"user_id\")\n",
        "    train_df = train_df.set_index([\"user_id\", \"edition_id\"]).reindex(\n",
        "        list(zip(X[\"user_id\"], X[\"edition_id\"]))\n",
        "    ).reset_index()\n",
        "    y = train_df[\"rel\"].values\n",
        "    groups = X.groupby(\"user_id\").size().values\n",
        "    \n",
        "    X_mat = X[available]\n",
        "    \n",
        "    if HAS_LGB:\n",
        "        model = lgb.LGBMRanker(\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.05,\n",
        "            num_leaves=31,\n",
        "            min_child_samples=20,\n",
        "            random_state=RANDOM_STATE,\n",
        "            verbose=-1,\n",
        "        )\n",
        "        model.fit(X_mat, y, group=groups)\n",
        "        return model, available\n",
        "    return None, available\n",
        "\n",
        "\n",
        "def predict_scores(model, candidates_df: pd.DataFrame, interactions: pd.DataFrame,\n",
        "                   edition_to_genres: Dict, edition_meta: dict, feats: List[str]):\n",
        "    \"\"\"Score each (user, edition) in candidates.\"\"\"\n",
        "    X = build_features(candidates_df, interactions, edition_to_genres, edition_meta)\n",
        "    available = [f for f in feats if f in X.columns]\n",
        "    if not available:\n",
        "        return np.ones(len(candidates_df))\n",
        "    X_mat = X[available]\n",
        "    return model.predict(X_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48718e1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 8. Diversity-Aware Re-ranking (MMR)\n",
        "\n",
        "# %%\n",
        "def mmr_rerank(\n",
        "    candidates: List[int],\n",
        "    scores: np.ndarray,\n",
        "    edition_to_genres: Dict[int, set],\n",
        "    top_k: int = 20,\n",
        "    lambda_param: float = 0.5,\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Maximal Marginal Relevance: balances relevance (scores) and diversity (genre dissimilarity).\n",
        "    lambda_param: 0 = pure diversity, 1 = pure relevance\n",
        "    \"\"\"\n",
        "    if len(candidates) <= top_k:\n",
        "        return candidates\n",
        "    selected = []\n",
        "    remaining = list(range(len(candidates)))\n",
        "    score_arr = np.array(scores)\n",
        "    # normalize scores to [0,1]\n",
        "    if score_arr.max() > score_arr.min():\n",
        "        score_arr = (score_arr - score_arr.min()) / (score_arr.max() - score_arr.min())\n",
        "    \n",
        "    for _ in range(top_k):\n",
        "        best_idx = None\n",
        "        best_mmr = -np.inf\n",
        "        for idx in remaining:\n",
        "            rel = score_arr[idx]\n",
        "            cand = candidates[idx]\n",
        "            g_cand = edition_to_genres.get(cand, set())\n",
        "            # diversity: max distance to already selected\n",
        "            if not selected:\n",
        "                div = 1.0\n",
        "            else:\n",
        "                div = np.mean([\n",
        "                    jaccard_distance(g_cand, edition_to_genres.get(s, set()))\n",
        "                    for s in selected\n",
        "                ]) if g_cand else 0.0\n",
        "            mmr = lambda_param * rel + (1 - lambda_param) * div\n",
        "            if mmr > best_mmr:\n",
        "                best_mmr = mmr\n",
        "                best_idx = idx\n",
        "        if best_idx is None:\n",
        "            break\n",
        "        selected.append(candidates[best_idx])\n",
        "        remaining.remove(best_idx)\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1ff5f209",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 9. Full Pipeline: Train → Predict → Re-rank → Submit\n",
        "\n",
        "# %%\n",
        "def run_pipeline(\n",
        "    data: dict,\n",
        "    targets: pd.DataFrame,\n",
        "    candidates: pd.DataFrame,\n",
        "    mmr_lambda: float = 0.6,\n",
        "    use_mmr: bool = True,\n",
        "):\n",
        "    \"\"\"Train model, predict, re-rank with diversity, produce submission.\"\"\"\n",
        "    interactions = data.get(\"interactions\")\n",
        "    if interactions is None or interactions.empty:\n",
        "        print(\"No interactions - using random baseline\")\n",
        "        return random_baseline(candidates, targets)\n",
        "    \n",
        "    edition_to_genres = build_edition_genres(data[\"editions\"], data[\"book_genres\"])\n",
        "    edition_meta = build_edition_metadata(data[\"editions\"], data[\"authors\"])\n",
        "    \n",
        "    # Training data\n",
        "    train_pairs = create_training_pairs(interactions, edition_to_genres, n_neg_per_user=100)\n",
        "    if train_pairs is None or len(train_pairs) < 100:\n",
        "        print(\"Insufficient training data - using popularity baseline\")\n",
        "        return popularity_baseline(candidates, targets, interactions, edition_to_genres)\n",
        "    \n",
        "    # Train ranker\n",
        "    model, feats = train_ranker(train_pairs, interactions, edition_to_genres, edition_meta)\n",
        "    if model is None:\n",
        "        return popularity_baseline(candidates, targets, interactions, edition_to_genres)\n",
        "    \n",
        "    # Predict for each user's candidates\n",
        "    rows = []\n",
        "    for uid in targets[\"user_id\"]:\n",
        "        user_cands = candidates[candidates[\"user_id\"] == uid]\n",
        "        if user_cands.empty:\n",
        "            continue\n",
        "        cand_df = user_cands.copy()\n",
        "        scores = predict_scores(model, cand_df, interactions, edition_to_genres, edition_meta, feats)\n",
        "        cand_ids = cand_df[\"edition_id\"].tolist()\n",
        "        if use_mmr and edition_to_genres:\n",
        "            top20 = mmr_rerank(cand_ids, scores, edition_to_genres, N_TOP, mmr_lambda)\n",
        "        else:\n",
        "            top20 = [cand_ids[i] for i in np.argsort(scores)[::-1][:N_TOP]]\n",
        "        for r, eid in enumerate(top20, 1):\n",
        "            rows.append({\"user_id\": uid, \"edition_id\": eid, \"rank\": r})\n",
        "    \n",
        "    sub = pd.DataFrame(rows)\n",
        "    return sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9c8cb292",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 10. Baseline Fallbacks (when data or model unavailable)\n",
        "\n",
        "# %%\n",
        "def random_baseline(candidates: pd.DataFrame, targets: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Random ranking of 20 from 200 candidates per user.\"\"\"\n",
        "    rows = []\n",
        "    for uid in targets[\"user_id\"]:\n",
        "        cands = candidates[candidates[\"user_id\"] == uid][\"edition_id\"].tolist()\n",
        "        chosen = np.random.choice(cands, size=min(N_TOP, len(cands)), replace=False)\n",
        "        for r, eid in enumerate(chosen, 1):\n",
        "            rows.append({\"user_id\": uid, \"edition_id\": int(eid), \"rank\": r})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def popularity_baseline(\n",
        "    candidates: pd.DataFrame,\n",
        "    targets: pd.DataFrame,\n",
        "    interactions: pd.DataFrame,\n",
        "    edition_to_genres: Dict,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Rank by global popularity (reads + wishlists), then MMR for diversity.\"\"\"\n",
        "    if interactions is not None and not interactions.empty:\n",
        "        pop = interactions.groupby(\"edition_id\")[\"event_type\"].apply(\n",
        "            lambda x: (x == 2).sum() * 3 + (x == 1).sum()\n",
        "        ).to_dict()\n",
        "    else:\n",
        "        pop = {}\n",
        "    \n",
        "    rows = []\n",
        "    for uid in targets[\"user_id\"]:\n",
        "        cands = candidates[candidates[\"user_id\"] == uid][\"edition_id\"].tolist()\n",
        "        scores = np.array([pop.get(e, 0) for e in cands])\n",
        "        top20 = mmr_rerank(cands, scores, edition_to_genres or {}, N_TOP, lambda_param=0.5)\n",
        "        for r, eid in enumerate(top20, 1):\n",
        "            rows.append({\"user_id\": uid, \"edition_id\": eid, \"rank\": r})\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19711e0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 11. Run & Save Submission\n",
        "\n",
        "# %%\n",
        "submission = run_pipeline(data, targets, candidates, mmr_lambda=0.6, use_mmr=True)\n",
        "\n",
        "# Validate format\n",
        "assert submission.groupby(\"user_id\").size().min() == N_TOP\n",
        "assert submission[\"rank\"].between(1, N_TOP).all()\n",
        "assert len(submission) == len(targets) * N_TOP\n",
        "\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "print(f\"Saved to {SUBMISSION_PATH}, shape {submission.shape}\")\n",
        "submission.head(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068e07e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# 12. (Optional) Local Validation\n",
        "# Use this if you have a temporal split: build ground_truth from future interactions.\n",
        "\n",
        "# %%\n",
        "# Example: evaluate on a held-out set (adjust to your split logic)\n",
        "# gt = {(row[\"user_id\"], row[\"edition_id\"]): (3 if row[\"event_type\"]==2 else 1)\n",
        "#       for _, row in val_interactions.iterrows()}\n",
        "# score, ndcg, div = evaluate_submission(submission, gt, edition_to_genres, ALPHA, BETA)\n",
        "# print(f\"Score={score:.4f}, NDCG@20={ndcg:.4f}, Diversity@20={div:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
